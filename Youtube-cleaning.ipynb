{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdd2a40-cbf1-4643-81e4-515c70dca372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bc48b0-3912-439e-89df-61a1edfc3ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidation complete. Data saved to C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-consolidated.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing your CSV files\n",
    "directory_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the consolidated data\n",
    "consolidated_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and append its data to the consolidated DataFrame\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    consolidated_df = pd.concat([consolidated_df, df], ignore_index=True)\n",
    "\n",
    "# Specify the path for the consolidated CSV file\n",
    "consolidated_csv_path = os.path.join(directory_path, 'Youtube-consolidated.csv')\n",
    "\n",
    "# Write the consolidated DataFrame to a new CSV file\n",
    "consolidated_df.to_csv(consolidated_csv_path, index=False)\n",
    "\n",
    "print(f\"Consolidation complete. Data saved to {consolidated_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bac7e22-4a85-4287-a96a-954ddbd74a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12984\\3915884046.py:5: DtypeWarning: Columns (14,15,16,17,18,19,20,21,22,23,24,25,26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Order         User Name          Comment Date Like Count  \\\n",
      "0      1        @thing9561  2023-11-27T22:08:43Z          1   \n",
      "1      2    @mitchliam9328  2023-11-26T08:58:35Z          2   \n",
      "2      3  @user-xz3jy5jc8w  2023-11-26T01:42:57Z          0   \n",
      "3      4  @user-ys9fg4ol9s  2023-11-23T03:21:20Z          0   \n",
      "4      5      @alexsim8063  2023-11-21T12:42:53Z          0   \n",
      "\n",
      "                                             Comment  \\\n",
      "0  I think we just got to remember that Barbie wa...   \n",
      "1  It's satire against matriarchy and patriarchy,...   \n",
      "2  Actually,some said Barbie was just making an i...   \n",
      "3  So Barbie hates capitalism yet let's remember,...   \n",
      "4  This movie is a dangerous, woke, anti-motherho...   \n",
      "\n",
      "                                           User Link Unnamed: 6 Unnamed: 7  \\\n",
      "0  http://www.youtube.com/channel/UCXwoMF0v7XC5Mg...        NaN        NaN   \n",
      "1  http://www.youtube.com/channel/UCEulVAA56UqSs_...        NaN        NaN   \n",
      "2  http://www.youtube.com/channel/UCSgrgaokK8oA2-...        NaN        NaN   \n",
      "3  http://www.youtube.com/channel/UC3mjy6m5uTVMsW...        NaN        NaN   \n",
      "4  http://www.youtube.com/channel/UC1eEfRHWnq1PdK...        NaN        NaN   \n",
      "\n",
      "  Unnamed: 8 Unnamed: 9  ... Unnamed: 18 Unnamed: 19 Unnamed: 20 Unnamed: 21  \\\n",
      "0        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "1        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "2        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "3        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "4        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 22 Unnamed: 23 Unnamed: 24 Unnamed: 25 Unnamed: 26 Unnamed: 27  \n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "Cleaned data:\n",
      "   Order         User Name          Comment Date Like Count  \\\n",
      "0      1        @thing9561  2023-11-27T22:08:43Z          1   \n",
      "1      2    @mitchliam9328  2023-11-26T08:58:35Z          2   \n",
      "2      3  @user-xz3jy5jc8w  2023-11-26T01:42:57Z          0   \n",
      "3      4  @user-ys9fg4ol9s  2023-11-23T03:21:20Z          0   \n",
      "4      5      @alexsim8063  2023-11-21T12:42:53Z          0   \n",
      "\n",
      "                                             Comment  \\\n",
      "0  i think we just got to remember that barbie wa...   \n",
      "1  its satire against matriarchy and patriarchy m...   \n",
      "2  actuallysome said barbie was just making an im...   \n",
      "3  so barbie hates capitalism yet lets remember  ...   \n",
      "4  this movie is a dangerous woke antimotherhood ...   \n",
      "\n",
      "                                           User Link Unnamed: 6 Unnamed: 7  \\\n",
      "0  http://www.youtube.com/channel/UCXwoMF0v7XC5Mg...        NaN        NaN   \n",
      "1  http://www.youtube.com/channel/UCEulVAA56UqSs_...        NaN        NaN   \n",
      "2  http://www.youtube.com/channel/UCSgrgaokK8oA2-...        NaN        NaN   \n",
      "3  http://www.youtube.com/channel/UC3mjy6m5uTVMsW...        NaN        NaN   \n",
      "4  http://www.youtube.com/channel/UC1eEfRHWnq1PdK...        NaN        NaN   \n",
      "\n",
      "  Unnamed: 8 Unnamed: 9  ... Unnamed: 18 Unnamed: 19 Unnamed: 20 Unnamed: 21  \\\n",
      "0        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "1        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "2        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "3        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "4        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 22 Unnamed: 23 Unnamed: 24 Unnamed: 25 Unnamed: 26 Unnamed: 27  \n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the consolidated CSV file with the appropriate encoding\n",
    "csv_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-consolidated_78981.csv'\n",
    "df = pd.read_csv(csv_path, encoding='latin1')\n",
    "\n",
    "# Display the first few rows to inspect the data\n",
    "print(df.head())\n",
    "\n",
    "# Handling missing values\n",
    "df.dropna(subset=['Comment'], inplace=True)  # Drop rows with missing comments\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=['Comment'], keep='first', inplace=True)\n",
    "\n",
    "# Preprocess text data (example: lowercase, remove special characters, etc.)\n",
    "df['Comment'] = df['Comment'].str.lower()\n",
    "df['Comment'] = df['Comment'].replace('[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_csv_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-consolidated_cleaned.csv'\n",
    "df.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(\"Cleaned data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4bd96b-1a43-4b66-ba4f-7b91ee85d709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/55.1 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/55.1 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/55.1 kB ? eta -:--:--\n",
      "     --------------------- ---------------- 30.7/55.1 kB 217.9 kB/s eta 0:00:01\n",
      "     --------------------- ---------------- 30.7/55.1 kB 217.9 kB/s eta 0:00:01\n",
      "     --------------------- ---------------- 30.7/55.1 kB 217.9 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 51.2/55.1 kB 175.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 55.1/55.1 kB 179.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.5 MB 640.0 kB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.1/1.5 MB 544.7 kB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.1/1.5 MB 435.7 kB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.1/1.5 MB 435.7 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/1.5 MB 374.1 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.1/1.5 MB 399.4 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.1/1.5 MB 425.3 kB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 436.9 kB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 461.0 kB/s eta 0:00:03\n",
      "     ------ --------------------------------- 0.3/1.5 MB 506.7 kB/s eta 0:00:03\n",
      "     ------- -------------------------------- 0.3/1.5 MB 575.5 kB/s eta 0:00:03\n",
      "     --------- ------------------------------ 0.4/1.5 MB 618.4 kB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.4/1.5 MB 639.0 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.5/1.5 MB 685.8 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.5/1.5 MB 756.4 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.6/1.5 MB 846.5 kB/s eta 0:00:02\n",
      "     ------------------- -------------------- 0.8/1.5 MB 919.5 kB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.9/1.5 MB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.1/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 133.4/133.4 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.8/58.8 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.6/42.6 kB ? eta 0:00:00\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 0.0/53.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.6/53.6 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 0.0/65.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.0/65.0 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17461 sha256=8270361097807e7ecd8d1a1c68038a0466f1ae0e9f26143104812b357b34d3f9\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\39\\17\\6f\\66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: hyperframe\n",
      "    Found existing installation: hyperframe 6.0.1\n",
      "    Uninstalling hyperframe-6.0.1:\n",
      "      Successfully uninstalled hyperframe-6.0.1\n",
      "  Attempting uninstall: hpack\n",
      "    Found existing installation: hpack 4.0.0\n",
      "    Uninstalling hpack-4.0.0:\n",
      "      Successfully uninstalled hpack-4.0.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: h2\n",
      "    Found existing installation: h2 4.1.0\n",
      "    Uninstalling h2-4.1.0:\n",
      "      Successfully uninstalled h2-4.1.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.25.1\n",
      "    Uninstalling httpx-0.25.1:\n",
      "      Successfully uninstalled httpx-0.25.1\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "youtube-search-python 1.6.6 requires httpx>=0.14.2, but you have httpx 0.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ada0bf5-508e-4e72-b434-097af5ee5559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537047a6-d234-48ad-b86f-8b8aa76a61ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now want to format the date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4053df2d-a3f5-4355-bbf3-b38e2b0912a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1069828303.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    df = pd.read_csv(C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv')\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the translated CSV file\n",
    "translated_csv_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-consolidated_translated.csv'\n",
    "df = pd.read_csv(translated_csv_path)\n",
    "\n",
    "# Convert only valid date values to datetime\n",
    "df['Comment Date'] = pd.to_datetime(df['Comment Date'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')\n",
    "\n",
    "# Extract the date and time into separate columns\n",
    "df['Date'] = df['Comment Date'].dt.date\n",
    "df['Time'] = df['Comment Date'].dt.time\n",
    "\n",
    "# Drop the original Comment Date column\n",
    "df.drop(columns=['Comment Date'], inplace=True)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "final_csv_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv'\n",
    "df.to_csv(final_csv_path, index=False)\n",
    "\n",
    "# Print statements for debugging\n",
    "print(\"Date and time columns added and unnecessary columns dropped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86193b93-34bf-49b3-b413-5af617dc3f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     - ------------------------------------ 30.7/981.5 kB 81.9 kB/s eta 0:00:12\n",
      "     - ------------------------------------ 30.7/981.5 kB 81.9 kB/s eta 0:00:12\n",
      "     - ------------------------------------ 41.0/981.5 kB 89.3 kB/s eta 0:00:11\n",
      "     - ------------------------------------ 41.0/981.5 kB 89.3 kB/s eta 0:00:11\n",
      "     - ------------------------------------ 41.0/981.5 kB 89.3 kB/s eta 0:00:11\n",
      "     - ------------------------------------ 41.0/981.5 kB 89.3 kB/s eta 0:00:11\n",
      "     -- ---------------------------------- 61.4/981.5 kB 112.8 kB/s eta 0:00:09\n",
      "     -- ---------------------------------- 61.4/981.5 kB 112.8 kB/s eta 0:00:09\n",
      "     --- --------------------------------- 81.9/981.5 kB 131.0 kB/s eta 0:00:07\n",
      "     --- --------------------------------- 92.2/981.5 kB 134.3 kB/s eta 0:00:07\n",
      "     ---- ------------------------------- 112.6/981.5 kB 155.9 kB/s eta 0:00:06\n",
      "     ---- ------------------------------- 122.9/981.5 kB 163.7 kB/s eta 0:00:06\n",
      "     ----- ------------------------------ 143.4/981.5 kB 185.1 kB/s eta 0:00:05\n",
      "     ------ ----------------------------- 163.8/981.5 kB 204.8 kB/s eta 0:00:04\n",
      "     ------ ----------------------------- 174.1/981.5 kB 209.5 kB/s eta 0:00:04\n",
      "     ------- ---------------------------- 204.8/981.5 kB 230.5 kB/s eta 0:00:04\n",
      "     --------- -------------------------- 245.8/981.5 kB 264.3 kB/s eta 0:00:03\n",
      "     ---------- ------------------------- 276.5/981.5 kB 283.8 kB/s eta 0:00:03\n",
      "     ----------- ------------------------ 317.4/981.5 kB 311.9 kB/s eta 0:00:03\n",
      "     ------------- ---------------------- 368.6/981.5 kB 347.3 kB/s eta 0:00:02\n",
      "     ---------------- ------------------- 440.3/981.5 kB 404.6 kB/s eta 0:00:02\n",
      "     ------------------ ----------------- 491.5/981.5 kB 433.6 kB/s eta 0:00:02\n",
      "     ------------------- ---------------- 532.5/981.5 kB 457.7 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 604.2/981.5 kB 506.8 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 686.1/981.5 kB 554.4 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 768.0/981.5 kB 606.2 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 860.2/981.5 kB 655.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 981.5/981.5 kB 722.6 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=a46f3d101306eb334971f3b4775379137936d14fd48380c1e13ce29e089e6732\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas googletrans==4.0.0-rc1 langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eba9450-f3b7-427a-aad6-ac5dde5a50dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c7260a-a3b3-4b29-a117-a4888f8599d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\3871529407.py:11: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download punkt resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your dataset\n",
    "data_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Example lexicon\n",
    "positive_lexicon = [\"empower\", \"inspire\", \"progress\", \"inclusive\", \"uplift\", \"positive\", \"equality\", \"feminist\", \"diverse\"]\n",
    "negative_lexicon = [\"anti-feminist\", \"stereotype\", \"sexist\", \"offensive\", \"controversial\", \"backward\", \"discriminate\", \"problematic\", \"regressive\"]\n",
    "\n",
    "# Tokenization and stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the Comment column\n",
    "df['Processed Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Sentiment analysis using lexicon\n",
    "def get_sentiment(tokens):\n",
    "    positive_count = sum(token in positive_lexicon for token in tokens)\n",
    "    negative_count = sum(token in negative_lexicon for token in tokens)\n",
    "\n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif positive_count < negative_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to each row\n",
    "df['Sentiment'] = df['Processed Comment'].apply(get_sentiment)\n",
    "\n",
    "# Save the results\n",
    "output_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53fe4c0-f710-4fa4-ab4d-fcd8aae3d145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\1713060580.py:11: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download punkt resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your dataset\n",
    "data_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Updated lexicon\n",
    "positive_lexicon = [\n",
    "    \"empowering\", \"inspirational\", \"progressive\", \"inclusive\", \"uplifting\", \"positive representation\",\n",
    "    \"equality\", \"strong female characters\", \"feminist\", \"diverse\", \"encouraging\", \"supportive\", \"liberating\",\n",
    "    \"empowered\", \"enlightening\", \"forward-thinking\", \"trailblazing\", \"visionary\", \"progressive ideals\",\n",
    "    \"progressive values\", \"progressive messages\", \"progressive themes\", \"affirmative\", \"constructive\",\n",
    "    \"open-minded\", \"tolerant\", \"accepting\", \"respecting\", \"progressive portrayal\", \"empowerment message\",\n",
    "    \"inspirational content\", \"progressive outlook\"\n",
    "]\n",
    "\n",
    "negative_lexicon = [\n",
    "    \"anti-feminist\", \"stereotypical\", \"sexist\", \"offensive\", \"controversial\", \"backward\", \"discriminatory\",\n",
    "    \"problematic\", \"regressive\", \"offensive portrayal\", \"stereotype reinforcement\", \"demeaning\", \"insensitive\",\n",
    "    \"disparaging\", \"offensive language\", \"negative representation\", \"bias\", \"pessimistic\", \"undermining\",\n",
    "    \"offensive content\", \"harmful\", \"offensive messages\", \"offensive themes\", \"counterproductive\", \"oppressive\",\n",
    "    \"repressive\", \"unprogressive\", \"unenlightened\", \"backward portrayal\", \"negative outlook\"\n",
    "]\n",
    "\n",
    "neutral_lexicon = [\n",
    "    \"movie\", \"character\", \"plot\", \"animation\", \"storyline\", \"director\", \"cinematography\", \"production\", \"release\",\n",
    "    \"audience\", \"entertainment\", \"screenplay\", \"cinematic\", \"cinematic elements\", \"visuals\", \"soundtrack\", \"editing\",\n",
    "    \"artistic\", \"technical aspects\", \"filmography\", \"performance\", \"acting\", \"performers\", \"cast\", \"cinematic techniques\",\n",
    "    \"production quality\", \"film industry\", \"film creation\", \"film direction\", \"film release\", \"viewer experience\",\n",
    "    \"audience response\", \"cinematic presentation\"\n",
    "]\n",
    "\n",
    "# Tokenization and stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the Comment column\n",
    "df['Processed Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Sentiment analysis using lexicon\n",
    "def get_sentiment(tokens):\n",
    "    positive_count = sum(token in positive_lexicon for token in tokens)\n",
    "    negative_count = sum(token in negative_lexicon for token in tokens)\n",
    "\n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif positive_count < negative_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to each row\n",
    "df['Sentiment'] = df['Processed Comment'].apply(get_sentiment)\n",
    "\n",
    "# Save the results\n",
    "output_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28afe88-0c9f-4c91-9960-8c3e16c78de2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Distribution:\n",
      "Neutral     72531\n",
      "Positive     3900\n",
      "Negative      358\n",
      "Name: Sentiment, dtype: int64\n",
      "\n",
      "Sentiment Percentages:\n",
      "Neutral     94.454935\n",
      "Positive     5.078852\n",
      "Negative     0.466213\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\3735523421.py:5: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_sentiment = pd.read_csv(sentiment_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentiment results CSV file\n",
    "sentiment_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df_sentiment = pd.read_csv(sentiment_path)\n",
    "\n",
    "# Count the occurrences of each sentiment\n",
    "sentiment_counts = df_sentiment['Sentiment'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each sentiment\n",
    "total_comments = len(df_sentiment)\n",
    "sentiment_percentages = (sentiment_counts / total_comments) * 100\n",
    "\n",
    "# Display the results\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\nSentiment Percentages:\")\n",
    "print(sentiment_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eddb1dc-40ca-4e39-92bc-113ce3f6db5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Comment Sentiment\n",
      "73566  i take both pills the unholy communion of shoe...   Neutral\n",
      "29318  have a great day barbie is a great player in t...   Neutral\n",
      "31492                                                NaN   Neutral\n",
      "34166                      why they pick such old actors   Neutral\n",
      "42570                        this looks fun very excited   Neutral\n",
      "9289   crazy how all the comments ignore the clear me...   Neutral\n",
      "10484  oh you dont like kenlet me introduce you to te...   Neutral\n",
      "40520                         this is beyond amazing lol   Neutral\n",
      "13842  just wanna say i watched the angry joe review ...   Neutral\n",
      "68420  understanding is outstanding coz everyone can ...   Neutral\n",
      "24269  this comment section is full of self claimed w...   Neutral\n",
      "27965  omfg soo excited i love barbie and my girls ar...   Neutral\n",
      "3549   i feel so opressed in my house that my husband...   Neutral\n",
      "72123  i was complemented on my yellow eyes once i ma...   Neutral\n",
      "32617                this sucks she is not barbie at all   Neutral\n",
      "57286  devils advocate here  isnt it a kids movie  an...   Neutral\n",
      "57461  i never ever trust a movie starring margo robb...  Positive\n",
      "1350   i think you missed the entire point of the mov...   Neutral\n",
      "55775  i thought that everybody knew this was gonna b...  Positive\n",
      "20293  does anyone remember how amy schumer refused t...   Neutral\n",
      "\n",
      "Sentiment Distribution:\n",
      "Neutral     72531\n",
      "Positive     3900\n",
      "Negative      358\n",
      "Name: Sentiment, dtype: int64\n",
      "\n",
      "Sentiment Percentages:\n",
      "Positive: 5.08%\n",
      "Neutral: 94.45%\n",
      "Negative: 0.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\2244791904.py:5: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_sentiment = pd.read_csv(sentiment_results_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentiment results CSV\n",
    "sentiment_results_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df_sentiment = pd.read_csv(sentiment_results_path)\n",
    "\n",
    "# Display a sample of comments and their sentiments\n",
    "sample_comments = df_sentiment.sample(20)  # Adjust the sample size as needed\n",
    "print(sample_comments[['Comment', 'Sentiment']])\n",
    "\n",
    "# Analyze sentiment distribution\n",
    "sentiment_distribution = df_sentiment['Sentiment'].value_counts()\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(sentiment_distribution)\n",
    "\n",
    "# Calculate sentiment percentages\n",
    "total_comments = len(df_sentiment)\n",
    "positive_percentage = (sentiment_distribution['Positive'] / total_comments) * 100\n",
    "neutral_percentage = (sentiment_distribution['Neutral'] / total_comments) * 100\n",
    "negative_percentage = (sentiment_distribution['Negative'] / total_comments) * 100\n",
    "\n",
    "print(\"\\nSentiment Percentages:\")\n",
    "print(f\"Positive: {positive_percentage:.2f}%\")\n",
    "print(f\"Neutral: {neutral_percentage:.2f}%\")\n",
    "print(f\"Negative: {negative_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "490232de-2546-445a-a780-327fe2ccf72f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\1423795162.py:7: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words:\n",
      "the      110881\n",
      "and       61389\n",
      "a         59887\n",
      "to        59371\n",
      "i         55793\n",
      "it        53648\n",
      "of        42383\n",
      "is        40025\n",
      "movi      36766\n",
      "that      36696\n",
      "thi       34099\n",
      "barbi     32436\n",
      "in        30174\n",
      "wa        23909\n",
      "be        23714\n",
      "you       21827\n",
      "for       21692\n",
      "but       17227\n",
      "like      16627\n",
      "with      16114\n",
      "dtype: int64\n",
      "Positive Words:\n",
      "['great']\n",
      "\n",
      "Negative Words:\n",
      "['bad']\n",
      "\n",
      "Neutral Words:\n",
      "['movie', 'character', 'plot', 'director']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load your dataset\n",
    "data_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Tokenization and stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the Comment column\n",
    "df['Processed Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Flatten the list of tokenized comments\n",
    "all_tokens = [token for comment_tokens in df['Processed Comment'] for token in comment_tokens]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = pd.Series(all_tokens).value_counts()\n",
    "\n",
    "# Display the top words\n",
    "top_words = word_frequencies.head(20)\n",
    "print(\"Top Words:\")\n",
    "print(top_words)\n",
    "\n",
    "# Manually classify words into positive, negative, or neutral\n",
    "positive_words = [\"inspirational\", \"positive\", \"uplifting\", \"great\", \"excellent\", \"innovative\", \"inspiring\"]\n",
    "negative_words = [\"offensive\", \"controversial\", \"regressive\", \"dreadful\", \"terrible\", \"horrible\", \"bad\"]\n",
    "neutral_words = [\"movie\", \"character\", \"plot\", \"animation\", \"storyline\", \"director\", \"production\", \"release\", \"audience\"]\n",
    "\n",
    "# Classify words based on lexicons\n",
    "classified_words = {\n",
    "    'Positive': [word for word in positive_words if word in word_frequencies.index],\n",
    "    'Negative': [word for word in negative_words if word in word_frequencies.index],\n",
    "    'Neutral': [word for word in neutral_words if word in word_frequencies.index],\n",
    "}\n",
    "\n",
    "# Display the classified words\n",
    "for sentiment, words in classified_words.items():\n",
    "    print(f\"{sentiment} Words:\")\n",
    "    print(words)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82fdbb12-4a43-4b01-8134-a3a9ca529529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\3361293276.py:7: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words:\n",
      "the      110881\n",
      "and       61389\n",
      "a         59887\n",
      "to        59371\n",
      "i         55793\n",
      "it        53648\n",
      "of        42383\n",
      "is        40025\n",
      "movi      36766\n",
      "that      36696\n",
      "thi       34099\n",
      "barbi     32436\n",
      "in        30174\n",
      "wa        23909\n",
      "be        23714\n",
      "you       21827\n",
      "for       21692\n",
      "but       17227\n",
      "like      16627\n",
      "with      16114\n",
      "they      15892\n",
      "not       15302\n",
      "are       15158\n",
      "have      14907\n",
      "as        14495\n",
      "so        14395\n",
      "just      13581\n",
      "on        12291\n",
      "about     12091\n",
      "ken       11328\n",
      "all       11001\n",
      "my        10750\n",
      "women     10651\n",
      "men       10107\n",
      "what       9936\n",
      "me         9559\n",
      "watch      8946\n",
      "how        8734\n",
      "see        8703\n",
      "at         8545\n",
      "if         8187\n",
      "think      8106\n",
      "your       8071\n",
      "go         7591\n",
      "or         7449\n",
      "world      7396\n",
      "film       7394\n",
      "she        7326\n",
      "im         7307\n",
      "make       7013\n",
      "dtype: int64\n",
      "Positive Words:\n",
      "['great']\n",
      "\n",
      "Negative Words:\n",
      "['bad', 'awful', 'hate']\n",
      "\n",
      "Neutral Words:\n",
      "['movie', 'character', 'plot', 'director', 'film', 'screenplay', 'soundtrack', 'cast']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load your dataset\n",
    "data_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Tokenization and stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the Comment column\n",
    "df['Processed Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Flatten the list of tokenized comments\n",
    "all_tokens = [token for comment_tokens in df['Processed Comment'] for token in comment_tokens]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_frequencies = pd.Series(all_tokens).value_counts()\n",
    "\n",
    "# Display the top words\n",
    "top_words = word_frequencies.head(50)\n",
    "print(\"Top Words:\")\n",
    "print(top_words)\n",
    "\n",
    "# Manually classify words into positive, negative, or neutral\n",
    "positive_words = [\"inspirational\", \"positive\", \"uplifting\", \"great\", \"excellent\", \"innovative\", \"inspiring\", \"fantastic\", \"amazing\", \"awesome\"]\n",
    "negative_words = [\"offensive\", \"controversial\", \"regressive\", \"dreadful\", \"terrible\", \"horrible\", \"bad\", \"disappointing\", \"awful\", \"hate\"]\n",
    "neutral_words = [\"movie\", \"character\", \"plot\", \"animation\", \"storyline\", \"director\", \"production\", \"release\", \"audience\", \"film\", \"entertainment\", \"screenplay\", \"cinematic\", \"visuals\", \"soundtrack\", \"editing\", \"artistic\", \"performance\", \"acting\", \"cast\"]\n",
    "\n",
    "# Classify words based on lexicons\n",
    "classified_words = {\n",
    "    'Positive': [word for word in positive_words if word in word_frequencies.index],\n",
    "    'Negative': [word for word in negative_words if word in word_frequencies.index],\n",
    "    'Neutral': [word for word in neutral_words if word in word_frequencies.index],\n",
    "}\n",
    "\n",
    "# Display the classified words\n",
    "for sentiment, words in classified_words.items():\n",
    "    print(f\"{sentiment} Words:\")\n",
    "    print(words)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a71515a-3386-461c-8c76-2b5d96d31705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Try again with expanded sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8d08de0-3c5b-4fb2-91a3-4ad8fc84a078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\2646424985.py:11: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download punkt resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your dataset\n",
    "data_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Updated lexicon\n",
    "positive_lexicon = [\n",
    "    \"empowering\", \"inspirational\", \"progressive\", \"inclusive\", \"uplifting\", \"positive representation\",\n",
    "    \"equality\", \"strong female characters\", \"feminist\", \"diverse\", \"encouraging\", \"supportive\", \"liberating\",\n",
    "    \"empowered\", \"enlightening\", \"forward-thinking\", \"trailblazing\", \"visionary\", \"progressive ideals\",\n",
    "    \"progressive values\", \"progressive messages\", \"progressive themes\", \"affirmative\", \"constructive\",\n",
    "    \"open-minded\", \"tolerant\", \"accepting\", \"respecting\", \"progressive portrayal\", \"empowerment message\",\n",
    "    \"inspirational content\", \"progressive outlook\", \"visionary\", \"trailblazing\", \"empowering\", \"innovative\",\n",
    "    \"optimistic\", \"hopeful\", \"upbeat\", \"empowering\", \"encouragement\", \"positive vibes\", \"positive energy\"\n",
    "]\n",
    "\n",
    "negative_lexicon = [\n",
    "    \"anti-feminist\", \"stereotypical\", \"sexist\", \"offensive\", \"controversial\", \"backward\", \"discriminatory\",\n",
    "    \"problematic\", \"regressive\", \"offensive portrayal\", \"stereotype reinforcement\", \"demeaning\", \"insensitive\",\n",
    "    \"disparaging\", \"offensive language\", \"negative representation\", \"bias\", \"pessimistic\", \"undermining\",\n",
    "    \"offensive content\", \"harmful\", \"offensive messages\", \"offensive themes\", \"counterproductive\", \"oppressive\",\n",
    "    \"repressive\", \"unprogressive\", \"unenlightened\", \"backward portrayal\", \"negative outlook\", \"depressing\",\n",
    "    \"disheartening\", \"discouraging\", \"hopeless\", \"pessimistic\", \"negative vibes\", \"negative energy\"\n",
    "]\n",
    "\n",
    "neutral_lexicon = [\n",
    "    \"movie\", \"character\", \"plot\", \"animation\", \"storyline\", \"director\", \"cinematography\", \"production\", \"release\",\n",
    "    \"audience\", \"entertainment\", \"screenplay\", \"cinematic\", \"cinematic elements\", \"visuals\", \"soundtrack\", \"editing\",\n",
    "    \"artistic\", \"technical aspects\", \"filmography\", \"performance\", \"acting\", \"performers\", \"cast\", \"cinematic techniques\",\n",
    "    \"production quality\", \"film industry\", \"film creation\", \"film direction\", \"film release\", \"viewer experience\",\n",
    "    \"audience response\", \"cinematic presentation\", \"generic\", \"standard\", \"typical\", \"common\", \"average\", \"ordinary\",\n",
    "    \"conventional\", \"usual\", \"routine\", \"traditional\", \"regular\", \"normal\", \"ordinary\"\n",
    "]\n",
    "\n",
    "# Tokenization and stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the Comment column\n",
    "df['Processed Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Sentiment analysis using lexicon\n",
    "def get_sentiment(tokens):\n",
    "    positive_count = sum(token in positive_lexicon for token in tokens)\n",
    "    negative_count = sum(token in negative_lexicon for token in tokens)\n",
    "\n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif positive_count < negative_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to each row\n",
    "df['Sentiment'] = df['Processed Comment'].apply(get_sentiment)\n",
    "\n",
    "# Save the results\n",
    "output_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f022966d-7608-499d-9e9d-521b70bad786",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Comment Sentiment\n",
      "68633  when i visited the theatre i was asked my iq a...   Neutral\n",
      "16733  christian but cant live under a rock i will go...   Neutral\n",
      "51573  at least with these films we get to see who al...   Neutral\n",
      "22067  omg my childhood and my barbies\\nthe song real...   Neutral\n",
      "28556                              when does it come out   Neutral\n",
      "30599                                          like grah   Neutral\n",
      "20846                       esse filme e muito engraado    Neutral\n",
      "49585  the wildest part of this movie was having a sc...   Neutral\n",
      "74243            absolute giga chad take from madam shoe   Neutral\n",
      "25118  i saw the whole movie just by watching the tra...   Neutral\n",
      "23709                                          no thanks   Neutral\n",
      "12940  is greta shy or just being overly careful abou...   Neutral\n",
      "63955  the scene when little girls smashing babydolls...   Neutral\n",
      "9836   why is feminism contrast to capitalism woman d...   Neutral\n",
      "255          so glad i didnt waste my time on this movie   Neutral\n",
      "30919                        is the movie this year 2023   Neutral\n",
      "38773                                why am i so excited   Neutral\n",
      "33634                             this im going to watch   Neutral\n",
      "10930                  women dont get paid less than men   Neutral\n",
      "44919                                              gasp    Neutral\n",
      "\n",
      "Sentiment Distribution:\n",
      "Neutral     72513\n",
      "Positive     3903\n",
      "Negative      373\n",
      "Name: Sentiment, dtype: int64\n",
      "\n",
      "Sentiment Percentages:\n",
      "Positive: 5.08%\n",
      "Neutral: 94.43%\n",
      "Negative: 0.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\2244791904.py:5: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_sentiment = pd.read_csv(sentiment_results_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentiment results CSV\n",
    "sentiment_results_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df_sentiment = pd.read_csv(sentiment_results_path)\n",
    "\n",
    "# Display a sample of comments and their sentiments\n",
    "sample_comments = df_sentiment.sample(20)  # Adjust the sample size as needed\n",
    "print(sample_comments[['Comment', 'Sentiment']])\n",
    "\n",
    "# Analyze sentiment distribution\n",
    "sentiment_distribution = df_sentiment['Sentiment'].value_counts()\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(sentiment_distribution)\n",
    "\n",
    "# Calculate sentiment percentages\n",
    "total_comments = len(df_sentiment)\n",
    "positive_percentage = (sentiment_distribution['Positive'] / total_comments) * 100\n",
    "neutral_percentage = (sentiment_distribution['Neutral'] / total_comments) * 100\n",
    "negative_percentage = (sentiment_distribution['Negative'] / total_comments) * 100\n",
    "\n",
    "print(\"\\nSentiment Percentages:\")\n",
    "print(f\"Positive: {positive_percentage:.2f}%\")\n",
    "print(f\"Neutral: {neutral_percentage:.2f}%\")\n",
    "print(f\"Negative: {negative_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be73bc81-6d01-4c9b-b671-b3a95097e69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Try new keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0511e4a6-84c6-43e5-804c-c2ba83790dae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21940\\2782543486.py:11: DtypeWarning: Columns (15,16,17,18,19,20,21,22,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download punkt resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your dataset\n",
    "data_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-final_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Updated lexicon\n",
    "positive_lexicon = [\n",
    "    \"empowering\", \"inspirational\", \"progressive\", \"inclusive\", \"uplifting\", \"positive representation\",\n",
    "    \"equality\", \"strong female characters\", \"feminist\", \"diverse\", \"encouraging\", \"supportive\", \"liberating\",\n",
    "    \"empowered\", \"enlightening\", \"forward-thinking\", \"trailblazing\", \"visionary\", \"progressive ideals\",\n",
    "    \"progressive values\", \"progressive messages\", \"progressive themes\", \"affirmative\", \"constructive\",\n",
    "    \"open-minded\", \"tolerant\", \"accepting\", \"respecting\", \"progressive portrayal\", \"empowerment message\",\n",
    "    \"inspirational content\", \"progressive outlook\", \"visionary\", \"trailblazing\", \"innovative\", \"optimistic\",\n",
    "    \"hopeful\", \"upbeat\", \"encouragement\", \"positive vibes\", \"positive energy\", \"like\", \"love\", \"enjoy\", \"good\",\n",
    "    \"amazing\", \"excellent\", \"awesome\", \"fantastic\", \"outstanding\", \"happy\", \"pleasurable\", \"satisfying\", \"joyful\",\n",
    "    \"content\", \"pleasant\", \"delightful\", \"fulfilling\", \"charming\", \"enchanting\", \"heartwarming\", \"captivating\",\n",
    "    \"magical\", \"wholesome\", \"sweet\", \"adorable\", \"fun\", \"imaginative\", \"beautiful\", \"well-crafted\",\n",
    "    \"engaging\", \"colorful\", \"inspirational\", \"heartwarming\", \"uplifting\", \"touching\", \"inspiring\", \"enchanting\",\n",
    "    \"moving\", \"charming\", \"sentimental\", \"tender\", \"nostalgic\", \"wholesome\", \"joyful\", \"empathetic\", \"beautiful\",\n",
    "    \"emotional journey\", \"tearjerker\", \"captivating\", \"feel-good\", \"radiant\", \"poignant\"\n",
    "]\n",
    "\n",
    "negative_lexicon = [\n",
    "    \"anti-feminist\", \"stereotypical\", \"sexist\", \"offensive\", \"controversial\", \"backward\", \"discriminatory\",\n",
    "    \"problematic\", \"regressive\", \"offensive portrayal\", \"stereotype reinforcement\", \"demeaning\", \"insensitive\",\n",
    "    \"disparaging\", \"offensive language\", \"negative representation\", \"bias\", \"pessimistic\", \"undermining\",\n",
    "    \"offensive content\", \"harmful\", \"offensive messages\", \"offensive themes\", \"counterproductive\", \"oppressive\",\n",
    "    \"repressive\", \"unprogressive\", \"unenlightened\", \"backward portrayal\", \"negative outlook\", \"depressing\",\n",
    "    \"disheartening\", \"discouraging\", \"hopeless\", \"pessimistic\", \"negative vibes\", \"dislike\", \"hate\", \"displeasure\",\n",
    "    \"unpleasant\", \"disappointing\", \"awful\", \"horrible\", \"terrible\", \"poor\", \"clichd\", \"predictable\",\n",
    "    \"overly simplistic\", \"unoriginal\", \"cheesy\", \"formulaic\", \"lacking depth\", \"corny\", \"childish\", \"unrealistic\",\n",
    "    \"mediocre\", \"dull\", \"repetitive\", \"uninspiring\", \"shallow\", \"boring\", \"confusing\", \"poorly executed\", \"contrived\",\n",
    "    \"superficial empowerment\", \"melodramatic\", \"saccharine\", \"manipulative\", \"mawkish\", \"sappy\", \"forced emotion\",\n",
    "    \"overemotional\", \"trite\", \"predictable emotions\", \"schmaltzy\", \"excessive pathos\", \"senti-mental\",\n",
    "    \"unconvincing emotion\", \"lack of emotional depth\", \"forced nostalgia\", \"unearned sentiment\"\n",
    "]\n",
    "\n",
    "neutral_lexicon = [\n",
    "    \"movie\", \"character\", \"plot\", \"animation\", \"storyline\", \"director\", \"cinematography\", \"production\", \"release\",\n",
    "    \"audience\", \"entertainment\", \"screenplay\", \"cinematic\", \"cinematic elements\", \"visuals\", \"soundtrack\", \"editing\",\n",
    "    \"artistic\", \"technical aspects\", \"filmography\", \"performance\", \"acting\", \"performers\", \"cast\", \"cinematic techniques\",\n",
    "    \"production quality\", \"film industry\", \"film creation\", \"film direction\", \"film release\", \"viewer experience\",\n",
    "    \"audience response\", \"cinematic presentation\", \"generic\", \"standard\", \"typical\", \"common\", \"average\", \"ordinary\",\n",
    "    \"conventional\", \"usual\", \"routine\", \"traditional\", \"regular\", \"normal\", \"ordinary\"\n",
    "]\n",
    "\n",
    "# Tokenization and stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the Comment column\n",
    "df['Processed Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Sentiment analysis using lexicon\n",
    "def get_sentiment(tokens):\n",
    "    positive_count = sum(token in positive_lexicon for token in tokens)\n",
    "    negative_count = sum(token in negative_lexicon for token in tokens)\n",
    "\n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif positive_count < negative_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to each row\n",
    "df['Sentiment'] = df['Processed Comment'].apply(get_sentiment)\n",
    "\n",
    "# Save the results\n",
    "output_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7339b844-4095-4594-90a6-409c9bdfafbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Comment Sentiment\n",
      "35278               glad this la isnt created by netflix   Neutral\n",
      "14873  the fact that the publicat least hispanic amer...  Negative\n",
      "72580                    officelamps wwhheeeeeeyyyyyyyyy   Neutral\n",
      "20807                timo filme recomendo assistirem ele   Neutral\n",
      "57682  will this movie spawn more single mothers i wo...   Neutral\n",
      "19852  youre gonna need bleach for your eyes after wa...   Neutral\n",
      "42654                             it looks awesome to me   Neutral\n",
      "18931                               0429  baahaha i lold   Neutral\n",
      "17638  this is a very dangerous situation i believe t...   Neutral\n",
      "49548  toy story 3 did a better barbie and ken story ...   Neutral\n",
      "43602                                    absolute cinema   Neutral\n",
      "20450   httpwwwyoutubecomchannelucxrntsi7sbpnrlbeb1zjhew   Neutral\n",
      "43481                                barbie  oppenheimer   Neutral\n",
      "31948  its funny that mattel is portrayed as an evil ...   Neutral\n",
      "19872  well this film was going to be woke i just ass...  Positive\n",
      "62532  could have been worse apparently amy schumer w...   Neutral\n",
      "61587  soooooo a typical modern hollywood movie ok go...   Neutral\n",
      "71264                personally i like the movie reviews  Positive\n",
      "64884  i liked the film though i do know it was not p...  Positive\n",
      "71610  i hate to be that person but i think everyone ...  Positive\n",
      "\n",
      "Sentiment Distribution:\n",
      "Neutral     52820\n",
      "Positive    22084\n",
      "Negative     1885\n",
      "Name: Sentiment, dtype: int64\n",
      "\n",
      "Sentiment Percentages:\n",
      "Positive: 28.76%\n",
      "Neutral: 68.79%\n",
      "Negative: 2.45%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentiment results CSV\n",
    "sentiment_results_path = r'C:\\Users\\user\\Desktop\\Miki\\Data\\Youtube_20231130\\Youtube-sentiment_results.csv'\n",
    "df_sentiment = pd.read_csv(sentiment_results_path)\n",
    "\n",
    "# Display a sample of comments and their sentiments\n",
    "sample_comments = df_sentiment.sample(20)  # Adjust the sample size as needed\n",
    "print(sample_comments[['Comment', 'Sentiment']])\n",
    "\n",
    "# Analyze sentiment distribution\n",
    "sentiment_distribution = df_sentiment['Sentiment'].value_counts()\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(sentiment_distribution)\n",
    "\n",
    "# Calculate sentiment percentages\n",
    "total_comments = len(df_sentiment)\n",
    "positive_percentage = (sentiment_distribution['Positive'] / total_comments) * 100\n",
    "neutral_percentage = (sentiment_distribution['Neutral'] / total_comments) * 100\n",
    "negative_percentage = (sentiment_distribution['Negative'] / total_comments) * 100\n",
    "\n",
    "print(\"\\nSentiment Percentages:\")\n",
    "print(f\"Positive: {positive_percentage:.2f}%\")\n",
    "print(f\"Neutral: {neutral_percentage:.2f}%\")\n",
    "print(f\"Negative: {negative_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba8486-5c61-4f3b-ab8a-f0621041db85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
